{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of Airflow\n",
    "================\n",
    "\n",
    "This notebook serves to show how operations provided by the toolbox can be applied.\n",
    "For this purpose, the following aspects are covered:\n",
    "\n",
    "* Access to ESA CCI Ozone and Cloud data (Atmosphere Mole Content of Ozone and Cloud Cover)\n",
    "* Geometric adjustments (coregistration)\n",
    "* Spatial (point, polygon) and temporal subsetting\n",
    "* Visualisation of time series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Installation\n",
    "\n",
    "Install Airflow from conda using\n",
    "\n",
    "`conda install airflow`\n",
    "\n",
    "We use `dag-factory` to generate the dags using `.yml` files.\n",
    "\n",
    "To install `dag-factory`, run:\n",
    "\n",
    "`pip install dag-factory`\n",
    "\n",
    "Now, before we run the Airflow server and dag-factory, we need to do some setup:\n",
    "\n",
    "1. Export these variables, make sure you are running it from the root of this directory:\n",
    "\n",
    "\n",
    "   ```\n",
    "   export AIRFLOW_HOME=$(pwd)\n",
    "   export AIRFLOW__CORE__LOAD_EXAMPLES=false\n",
    "   ```\n",
    "\n",
    "2. Now, create a `dags` folder in the root directory and add the `generate_dags.py` and `dag_config.yml`(already done in this PR).\n",
    "\n",
    "   P.S. -> Airflow executes the file `generate_dags` file which reads the config file and generates dags on the fly which can be seen in the UI.\n",
    "\n",
    "   To read more about the config file, see [here](https://github.com/bcdev/gaiaflow/blob/main/%7B%7B%20cookiecutter.folder_name%20%7D%7D/dags/change_me_config.yml)\n",
    "\n",
    "3. Now you have a basic Airflow DAG ready which can be seen and triggered inside the Airflow UI. To run the server, open your terminal from the root directory and run\n",
    "\n",
    "    `airflow standalone`\n",
    "\n",
    "4. The Airflow server should now run at [http://localhost:8080](http://localhost:8080). You will be prompted with authentication, use `admin` as the username and for the password, look into the logs on your terminal where you ran the command above.\n",
    "\n",
    "5. Once you are logged in, you can find the dag `test_cci` that was created from the config file defined in the `dags` folder.\n",
    "\n",
    "6. **Next steps** Create a python module that does one complete task i.e. arguments provide it everything it needs and it outputs what the next task needs, and then you can create the task dependencies and task groups (this is useful when you have several tasks that logically are meant to be computed together as a group, it could be useful for running the same operator on several datasets.).\n",
    "\n",
    "\n",
    "Some Ideas:\n",
    "\n",
    "- Use task groups for same operator on several datasets requirement (all the stats from each datasets have to be saved and accessible to the next task (if needed)).\n",
    "- Use dependencies for operator chaining (but this will involve saving the data for every task)\n",
    "\n",
    "------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
